model_type: gpt2
config_overrides:
  vocab_size: 50304
  n_embd: 1280
  n_head: 20
  n_layer: 36
  resid_pdrop: 0.3
  embd_pdrop: 0.3
  attn_pdrop: 0.3
tokenizer_name: HiTZ/gpt2-eus-euscrawl
dataset_name: HiTZ/euscrawl
dataset_config_name: default
validation_split_percentage: 0.1
output_dir: out/gpt2-large-eus-euscrawl
overwrite_output_dir: true
do_train: true
do_eval: true
evaluation_strategy: steps
per_device_train_batch_size: 30
per_device_eval_batch_size: 30
gradient_accumulation_steps: 4
learning_rate: 0.00025
weight_decay: 0.1
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
max_steps: 600000
lr_scheduler_type: cosine
warmup_steps: 2000
logging_steps: 10
save_steps: 500
seed: 42
bf16: true
eval_steps: 500
save_total_limit: 4
run_name: gpt2-large-eus-euscrawl
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false
optim: adamw_torch
report_to: wandb
ddp_find_unused_parameters: false
push_to_hub: true
resume_from_checkpoint: false
hub_model_id: HiTZ/gpt2-large-eus-euscrawl
hub_private_repo: true
torch_compile: true
