{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNri/luEnhgVMT5h2dHOJk7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Few-shot Learning with Multilingual Language Models (XGLM)"],"metadata":{"id":"8R0eCB_EtG5x"}},{"cell_type":"code","source":["!pip install fairseq\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K65uPzXVq-Tm","executionInfo":{"status":"ok","timestamp":1674596049386,"user_tz":-60,"elapsed":6677,"user":{"displayName":"Julen Etxaniz Aragoneses","userId":"06956422670240182492"}},"outputId":"87219c75-bb5b-4674-dda1-e97991358622"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"markdown","source":["## Introduction\n","\n","In this work, we train a family of multilingual generative language models, dubbed XGLM, on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning on more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (+7.4 accuracy points for 0-shot, +9.4 for 4-shot) and natural language inference (+5.4 for 0-shot, +5.4 for 4-shot). We have included a [model card](model_card.md) of XGLM for transparency and accountability.\n","\n"],"metadata":{"id":"njS2ycS8wu1N"}},{"cell_type":"markdown","source":["## Data and Languages\n","XGLM models are trained on a new multilingual corpus extracted from CommonCrawl (CC100-XL), a significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from [Summer 2013](http://commoncrawl.org/2013/11/new-crawl-data-available/) to [March/April 2020](https://commoncrawl.org/2020/04/march-april-2020-crawl-archive-now-available/) consisting of 134 languages. The detailed languages and data statistics are reported in the paper (Table A.1).\n","\n"],"metadata":{"id":"_kqwe5-gwxnQ"}},{"cell_type":"markdown","source":["## Pre-trained models\n","\n","Model | Layers | Model Dim | FFN Dim | Languages | Download\n","---|---|---|---|---|---\n","`XGLM 564M` | 24 | 1024 | 4096 | trained on 30 languages|  [xglm.564M.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.564M.tar.gz)\n","`XGLM 1.7B` | 24 | 2048 | 8192 | trained on 30 languages|  [xglm.1.7B.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.1.7B.tar.gz)\n","`XGLM 2.9B` | 48 | 2048 | 8192 | trained on 30 languages|  [xglm.2.9B.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.2.9B.tar.gz)\n","`XGLM 7.5B` | 32 | 4096 | 16384 | trained on 30 languages|  [xglm.7.5B.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.7.5B.tar.gz)\n","`XGLM 4.5B` | 48 | 2048 | 16384 | trained on 134 languages|  [xglm.4.5B.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.4.5B.tar.gz)"],"metadata":{"id":"Z0X4Mt19w09E"}},{"cell_type":"markdown","source":["## Pre-training Data Format\n","Our models were pre-trained with data in the following format (i.e. paragraphs are separated with new lines and documents were separated with double new lines).\n","```\n","<doc0,para0,tok0> ... <doc0,para0,tokX0> # X0: number of tokens in para0 of doc0\n","<doc0,para1,tok0> ... <doc0,para1,tokY0> # Y0: number of tokens in para1 of doc0\n","\n","<doc1,para0,tok0> ... <doc1,para0,tokX1> # X1: number of tokens in para0 of doc1\n","<doc1,para1,tok0> ... <doc1,para1,tokY1> # Y1: number of tokens in para1 of doc1\n","\n","...\n","```\n","Fairseq's preprocessing replaces newlines with the end-of-sentence symbol (`</s>`). As a result, the models never saw newline characters during pretraining and the same preprocessing should be run prior to few-shot inference to maximize performance. For example, our language model scoring function has `replace_newlines_with_eos` argument to trigger this preprocessing:"],"metadata":{"id":"KqFFQT_gtYUa"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"urjTm25Qp7vV","executionInfo":{"status":"ok","timestamp":1674596079473,"user_tz":-60,"elapsed":17938,"user":{"displayName":"Julen Etxaniz Aragoneses","userId":"06956422670240182492"}}},"outputs":[],"source":["from fairseq.models.transformer_lm import TransformerLanguageModel\n","\n","model_dir = 'https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.564M.tar.gz'\n","lm = TransformerLanguageModel.from_pretrained(model_dir, bpe='sentencepiece')\n","\n","text = \"\"\"First paragraph of the first document.\n","Second paragraph of the first document.\n","\n","First paragraph of the second document.\n","\"\"\"\n","tokens = lm.score(text, replace_newlines_with_eos=True)['tokens']\n","assert '\\n' not in lm.decode(tokens)  # no newlines were encoded"]},{"cell_type":"markdown","source":["## Evaluation\n","\n"],"metadata":{"id":"_eSAHAQYtcvd"}},{"cell_type":"markdown","source":["### Example (COPA)\n","\n","The following snippet show how to evaluate our models on the Choice of Plausible Alternatives (COPA) task, using examples in English, Chinese and Hindi."],"metadata":{"id":"xQkxbJXgwmc2"}},{"cell_type":"code","source":["data_samples = {\n","    'en': [\n","        {\n","            \"premise\": \"I wanted to conserve energy.\", \n","            \"choice1\": \"I swept the floor in the unoccupied room.\", \n","            \"choice2\": \"I shut off the light in the unoccupied room.\",\n","            \"question\": \"effect\",\n","            \"label\": \"1\"\n","        },\n","        {\n","            \"premise\": \"The flame on the candle went out.\",\n","            \"choice1\": \"I blew on the wick.\", \n","            \"choice2\": \"I put a match to the wick.\",\n","            \"question\": \"cause\",\n","            \"label\": \"0\"\n","        }\n","    ],\n","    'zh': [\n","        {\n","            \"premise\": \"我想节约能源。\", \n","            \"choice1\": \"我在空着的房间里扫了地板。\", \n","            \"choice2\": \"我把空房间里的灯关了。\",\n","            \"question\": \"effect\",\n","            \"label\": \"1\"\n","        },\n","        {\n","            \"premise\": \"蜡烛上的火焰熄灭了。\",\n","            \"choice1\": \"我吹灭了灯芯。\", \n","            \"choice2\": \"我把一根火柴放在灯芯上。\",\n","            \"question\": \"cause\",\n","            \"label\": \"0\"\n","        }\n","    ],\n","    'hi': [\n","        {\n","            \"premise\": \"M te vle konsève enèji.\", \n","            \"choice1\": \"Mwen te fin baleye chanm lib la.\", \n","            \"choice2\": \"Mwen te femen limyè nan chanm lib la.\",\n","            \"question\": \"effect\",\n","            \"label\": \"1\"\n","        },\n","        {\n","            \"premise\": \"Flam bouji a te etenn.\",\n","            \"choice1\": \"Mwen te soufle bouji a.\", \n","            \"choice2\": \"Mwen te limen mèch bouji a.\",\n","            \"question\": \"cause\",\n","            \"label\": \"0\"\n","        }\n","    ]\n","}"],"metadata":{"id":"zWQ_jhWyrJW7","executionInfo":{"status":"ok","timestamp":1674596226827,"user_tz":-60,"elapsed":452,"user":{"displayName":"Julen Etxaniz Aragoneses","userId":"06956422670240182492"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["In this example, we format the examples use the non-verbal prompts `{premise}\\n{choice1}` and `{premise}\\n{choice2}`, which are shared by all three languages. "],"metadata":{"id":"vSp5OLOztmKU"}},{"cell_type":"code","source":["from fairseq.models.transformer_lm import TransformerLanguageModel\n","\n","model_dir = 'https://dl.fbaipublicfiles.com/fairseq/models/xglm/xglm.564M.tar.gz'\n","lm = TransformerLanguageModel.from_pretrained(model_dir, bpe='sentencepiece')\n","lm = lm.eval()\n","lm = lm.half()\n","lm = lm.cuda()\n","\n","def get_logprobs(prompt):\n","    import re\n","    prompt = re.sub('\\n+' , '\\n', prompt)  # collapse repeated newlines, which indicate separate documents\n","    return lm.score(prompt, replace_newlines_with_eos=True)['positional_scores']\n","    \n","# Zero-shot evaluation for the Choice of Plausible Alternatives (COPA) task.\n","# A return value of 0 indicates that the first alternative is more plausible,\n","# while 1 indicates that the second alternative is more plausible.\n","def COPA_eval(prompt, alternative1, alternative2):\n","    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1).sum()\n","    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2).sum()\n","    return 0 if lprob1 > lprob2 else 1\n","    \n","for lang in ['en', 'zh', 'hi']:\n","    for idx, example in enumerate(data_samples[lang]):\n","        predict = COPA_eval(example[\"premise\"], example[\"choice1\"], example[\"choice2\"])\n","        print(f'{lang}-{idx}', predict, example['label'])\n","        \n","# en-0 1 1\n","# en-1 0 0\n","# zh-0 1 1\n","# zh-1 0 0\n","# hi-0 1 1\n","# hi-1 0 0"],"metadata":{"id":"fcWXeeVWr1ha"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### XCOPA"],"metadata":{"id":"wAgRzE1q3i4h"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","langs_xcopa = [\"et\", \"ht\", \"it\", \"id\", \"qu\", \"sw\", \"zh\", \"ta\", \"th\", \"tr\", \"vi\"]\n","\n","xcopa = {}\n","for lang in langs_xcopa:\n","    xcopa[lang] = load_dataset(\"xcopa\", lang)"],"metadata":{"id":"GoU5gVsA3le5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Zero-shot evaluation for the Choice of Plausible Alternatives (COPA) task.\n","# A return value of 0 indicates that the first alternative is more plausible,\n","# while 1 indicates that the second alternative is more plausible.\n","def COPA_eval(prompt, alternative1, alternative2):\n","    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1).sum()\n","    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2).sum()\n","    return 0 if lprob1 > lprob2 else 1\n","\n","for lang in langs_xcopa:\n","    for idx, example in enumerate(xcopa[lang][\"test\"]):\n","        predict = COPA_eval(example[\"premise\"], example[\"choice1\"], example[\"choice2\"])\n","        print(f'{lang}-{idx}', predict, example['label'])"],"metadata":{"id":"DlX9gOLB4ADG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### XStoryCloze"],"metadata":{"id":"qC0JButVykCt"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","langs = [\"en\", \"ru\", \"zh\", \"es\", \"ar\", \"hi\", \"id\", \"te\", \"sw\", \"eu\", \"my\"]\n","\n","x_story_cloze = {}\n","for lang in langs:\n","    x_story_cloze[lang] = load_dataset('x_story_cloze.py', lang)"],"metadata":{"id":"0AnmZM46zTVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def XStoryCloze_eval(prompt, alternative1, alternative2):\n","    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1).sum()\n","    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2).sum()\n","    return 0 if lprob1 > lprob2 else 1\n","\n","for lang in langs:\n","    for idx, example in enumerate(x_story_cloze[lang][\"eval\"]):\n","        input_sentences = example[\"input_sentence_1\"] + \"\\n\" + example[\"input_sentence_2\"] + \"\\n\" + example[\"input_sentence_3\"] + \"\\n\" + example[\"input_sentence_4\"]\n","        predict = XStoryCloze_eval(input_sentences, example[\"sentence_quiz1\"], example[\"sentence_quiz2\"])\n","        print(f'{lang}-{idx}', predict, example['label'])"],"metadata":{"id":"YFCbVQAEz6sh"},"execution_count":null,"outputs":[]}]}